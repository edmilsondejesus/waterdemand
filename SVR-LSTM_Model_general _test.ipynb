{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "naOfrff2U2Xy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random as rd\n",
        "import seaborn as sbs\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVR\n",
        "from matplotlib import pyplot\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow\n",
        "from tensorflow import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_data = 'DS_Agua_2017_2022.csv'\n",
        "path_name='data/'\n",
        "arquivo = f'{path_name}{file_data}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "JPemdWEukqf-"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(arquivo, sep =';', encoding = 'latin1', decimal='.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuOW4pP6kunV",
        "outputId": "b320cbcd-cb8c-4c5c-ca5d-2d45ea985ca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 45240 entries, 1 to 45240\n",
            "Data columns (total 7 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   DT_MEDICAO_HORA         45240 non-null  object \n",
            " 1   PRECIPITACAO            45240 non-null  float64\n",
            " 2   PRESSAO_ATMOSFERICA     45240 non-null  float64\n",
            " 3   TEMPERATURA_DO_AR_C     45240 non-null  float64\n",
            " 4   UMIDADE_RELATIVA_DO_AR  45240 non-null  float64\n",
            " 5   VELOCIDADE_VENTO        45240 non-null  float64\n",
            " 6   VL_MEDICAO              45240 non-null  float64\n",
            "dtypes: float64(6), object(1)\n",
            "memory usage: 2.8+ MB\n"
          ]
        }
      ],
      "source": [
        "dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o1Ml8Hwk11N",
        "outputId": "be1844c7-5af6-48d6-9212-9143305c451a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DT_MEDICAO_HORA           0\n",
              "PRECIPITACAO              0\n",
              "PRESSAO_ATMOSFERICA       0\n",
              "TEMPERATURA_DO_AR_C       0\n",
              "UMIDADE_RELATIVA_DO_AR    0\n",
              "VELOCIDADE_VENTO          0\n",
              "VL_MEDICAO                0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#verifica se existe variáveis nulas \n",
        "dataset.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "tWk39SDZk6q_",
        "outputId": "221b1b78-0ac9-44fa-a8cd-f8e66b48f198"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DT_MEDICAO_HORA</th>\n",
              "      <th>PRECIPITACAO</th>\n",
              "      <th>PRESSAO_ATMOSFERICA</th>\n",
              "      <th>TEMPERATURA_DO_AR_C</th>\n",
              "      <th>UMIDADE_RELATIVA_DO_AR</th>\n",
              "      <th>VELOCIDADE_VENTO</th>\n",
              "      <th>VL_MEDICAO</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-01-01 00:00:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1009.5</td>\n",
              "      <td>25.9</td>\n",
              "      <td>75.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>5016.845991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-01-01 01:00:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1009.6</td>\n",
              "      <td>25.7</td>\n",
              "      <td>76.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>4998.296902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-01-01 02:00:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1009.2</td>\n",
              "      <td>25.6</td>\n",
              "      <td>76.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>5013.330096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-01-01 03:00:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1008.4</td>\n",
              "      <td>25.9</td>\n",
              "      <td>77.0</td>\n",
              "      <td>1.7</td>\n",
              "      <td>4972.739746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2017-01-01 04:00:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1008.1</td>\n",
              "      <td>23.7</td>\n",
              "      <td>85.0</td>\n",
              "      <td>1.7</td>\n",
              "      <td>4921.942265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2017-01-01 05:00:00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1007.5</td>\n",
              "      <td>23.7</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>4838.136258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2017-01-01 06:00:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1007.3</td>\n",
              "      <td>24.2</td>\n",
              "      <td>88.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>4833.215005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2017-01-01 07:00:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1007.3</td>\n",
              "      <td>24.2</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>4795.247653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2017-01-01 08:00:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1007.7</td>\n",
              "      <td>24.7</td>\n",
              "      <td>85.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4763.098501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2017-01-01 09:00:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1008.5</td>\n",
              "      <td>25.1</td>\n",
              "      <td>83.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>4715.598748</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        DT_MEDICAO_HORA  PRECIPITACAO  PRESSAO_ATMOSFERICA  \\\n",
              "1   2017-01-01 00:00:00           0.0               1009.5   \n",
              "2   2017-01-01 01:00:00           0.0               1009.6   \n",
              "3   2017-01-01 02:00:00           0.0               1009.2   \n",
              "4   2017-01-01 03:00:00           0.0               1008.4   \n",
              "5   2017-01-01 04:00:00           0.0               1008.1   \n",
              "6   2017-01-01 05:00:00           0.2               1007.5   \n",
              "7   2017-01-01 06:00:00           0.0               1007.3   \n",
              "8   2017-01-01 07:00:00           0.0               1007.3   \n",
              "9   2017-01-01 08:00:00           0.0               1007.7   \n",
              "10  2017-01-01 09:00:00           0.0               1008.5   \n",
              "\n",
              "    TEMPERATURA_DO_AR_C  UMIDADE_RELATIVA_DO_AR  VELOCIDADE_VENTO   VL_MEDICAO  \n",
              "1                  25.9                    75.0               1.3  5016.845991  \n",
              "2                  25.7                    76.0               0.9  4998.296902  \n",
              "3                  25.6                    76.0               1.1  5013.330096  \n",
              "4                  25.9                    77.0               1.7  4972.739746  \n",
              "5                  23.7                    85.0               1.7  4921.942265  \n",
              "6                  23.7                    87.0               0.4  4838.136258  \n",
              "7                  24.2                    88.0               0.5  4833.215005  \n",
              "8                  24.2                    87.0               0.7  4795.247653  \n",
              "9                  24.7                    85.0               1.0  4763.098501  \n",
              "10                 25.1                    83.0               1.2  4715.598748  "
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "fiXB9plPTyJD",
        "outputId": "a0b80135-0cf1-41c3-e7e1-f9d88a4d3d58"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VL_MEDICAO</th>\n",
              "      <th>vl-1</th>\n",
              "      <th>vl-2</th>\n",
              "      <th>vl-3</th>\n",
              "      <th>tp-n</th>\n",
              "      <th>pr-n</th>\n",
              "      <th>vv-n</th>\n",
              "      <th>ur-n</th>\n",
              "      <th>ch-n</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4972.739746</td>\n",
              "      <td>5013.330096</td>\n",
              "      <td>4998.296902</td>\n",
              "      <td>5016.845991</td>\n",
              "      <td>25.9</td>\n",
              "      <td>1008.4</td>\n",
              "      <td>1.7</td>\n",
              "      <td>77.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4921.942265</td>\n",
              "      <td>4972.739746</td>\n",
              "      <td>5013.330096</td>\n",
              "      <td>4998.296902</td>\n",
              "      <td>23.7</td>\n",
              "      <td>1008.1</td>\n",
              "      <td>1.7</td>\n",
              "      <td>85.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4838.136258</td>\n",
              "      <td>4921.942265</td>\n",
              "      <td>4972.739746</td>\n",
              "      <td>5013.330096</td>\n",
              "      <td>23.7</td>\n",
              "      <td>1007.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4833.215005</td>\n",
              "      <td>4838.136258</td>\n",
              "      <td>4921.942265</td>\n",
              "      <td>4972.739746</td>\n",
              "      <td>24.2</td>\n",
              "      <td>1007.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>88.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4795.247653</td>\n",
              "      <td>4833.215005</td>\n",
              "      <td>4838.136258</td>\n",
              "      <td>4921.942265</td>\n",
              "      <td>24.2</td>\n",
              "      <td>1007.3</td>\n",
              "      <td>0.7</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4763.098501</td>\n",
              "      <td>4795.247653</td>\n",
              "      <td>4833.215005</td>\n",
              "      <td>4838.136258</td>\n",
              "      <td>24.7</td>\n",
              "      <td>1007.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4715.598748</td>\n",
              "      <td>4763.098501</td>\n",
              "      <td>4795.247653</td>\n",
              "      <td>4833.215005</td>\n",
              "      <td>25.1</td>\n",
              "      <td>1008.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>83.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4712.059896</td>\n",
              "      <td>4715.598748</td>\n",
              "      <td>4763.098501</td>\n",
              "      <td>4795.247653</td>\n",
              "      <td>26.2</td>\n",
              "      <td>1009.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>77.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4657.193811</td>\n",
              "      <td>4712.059896</td>\n",
              "      <td>4715.598748</td>\n",
              "      <td>4763.098501</td>\n",
              "      <td>28.4</td>\n",
              "      <td>1009.5</td>\n",
              "      <td>1.6</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4683.363206</td>\n",
              "      <td>4657.193811</td>\n",
              "      <td>4712.059896</td>\n",
              "      <td>4715.598748</td>\n",
              "      <td>28.7</td>\n",
              "      <td>1010.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4699.865601</td>\n",
              "      <td>4683.363206</td>\n",
              "      <td>4657.193811</td>\n",
              "      <td>4712.059896</td>\n",
              "      <td>31.6</td>\n",
              "      <td>1010.2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>57.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4716.826297</td>\n",
              "      <td>4699.865601</td>\n",
              "      <td>4683.363206</td>\n",
              "      <td>4657.193811</td>\n",
              "      <td>31.2</td>\n",
              "      <td>1009.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4715.140992</td>\n",
              "      <td>4716.826297</td>\n",
              "      <td>4699.865601</td>\n",
              "      <td>4683.363206</td>\n",
              "      <td>30.7</td>\n",
              "      <td>1009.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>4758.435488</td>\n",
              "      <td>4715.140992</td>\n",
              "      <td>4716.826297</td>\n",
              "      <td>4699.865601</td>\n",
              "      <td>31.8</td>\n",
              "      <td>1008.7</td>\n",
              "      <td>1.6</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4721.544583</td>\n",
              "      <td>4758.435488</td>\n",
              "      <td>4715.140992</td>\n",
              "      <td>4716.826297</td>\n",
              "      <td>31.1</td>\n",
              "      <td>1008.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>58.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     VL_MEDICAO         vl-1         vl-2         vl-3  tp-n    pr-n  vv-n  \\\n",
              "4   4972.739746  5013.330096  4998.296902  5016.845991  25.9  1008.4   1.7   \n",
              "5   4921.942265  4972.739746  5013.330096  4998.296902  23.7  1008.1   1.7   \n",
              "6   4838.136258  4921.942265  4972.739746  5013.330096  23.7  1007.5   0.4   \n",
              "7   4833.215005  4838.136258  4921.942265  4972.739746  24.2  1007.3   0.5   \n",
              "8   4795.247653  4833.215005  4838.136258  4921.942265  24.2  1007.3   0.7   \n",
              "9   4763.098501  4795.247653  4833.215005  4838.136258  24.7  1007.7   1.0   \n",
              "10  4715.598748  4763.098501  4795.247653  4833.215005  25.1  1008.5   1.2   \n",
              "11  4712.059896  4715.598748  4763.098501  4795.247653  26.2  1009.0   1.5   \n",
              "12  4657.193811  4712.059896  4715.598748  4763.098501  28.4  1009.5   1.6   \n",
              "13  4683.363206  4657.193811  4712.059896  4715.598748  28.7  1010.0   1.6   \n",
              "14  4699.865601  4683.363206  4657.193811  4712.059896  31.6  1010.2   1.5   \n",
              "15  4716.826297  4699.865601  4683.363206  4657.193811  31.2  1009.9   1.7   \n",
              "16  4715.140992  4716.826297  4699.865601  4683.363206  30.7  1009.5   1.4   \n",
              "17  4758.435488  4715.140992  4716.826297  4699.865601  31.8  1008.7   1.6   \n",
              "18  4721.544583  4758.435488  4715.140992  4716.826297  31.1  1008.0   1.8   \n",
              "\n",
              "    ur-n  ch-n  \n",
              "4   77.0   0.0  \n",
              "5   85.0   0.0  \n",
              "6   87.0   0.2  \n",
              "7   88.0   0.0  \n",
              "8   87.0   0.0  \n",
              "9   85.0   0.0  \n",
              "10  83.0   0.0  \n",
              "11  77.0   0.0  \n",
              "12  67.0   0.0  \n",
              "13  68.0   0.0  \n",
              "14  57.0   0.0  \n",
              "15  56.0   0.0  \n",
              "16  56.0   0.0  \n",
              "17  56.0   0.0  \n",
              "18  58.0   0.0  "
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#tratando o dataframe\n",
        "n_time_steps=3\n",
        "df = pd.DataFrame()\n",
        "df['VL_MEDICAO']=dataset['VL_MEDICAO']\n",
        "\n",
        "#transformação da série => janela deslizande com 1 passo de tempo\n",
        "for n_step in range(1,n_time_steps+1,1):\n",
        " df['vl-'+str(n_step)]=dataset['VL_MEDICAO'].shift(n_step)  \n",
        " \n",
        "df['tp-n']=dataset['TEMPERATURA_DO_AR_C']\n",
        "df['pr-n']=dataset['PRESSAO_ATMOSFERICA']\n",
        "df['vv-n']=dataset['VELOCIDADE_VENTO']\n",
        "df['ur-n']=dataset['UMIDADE_RELATIVA_DO_AR']\n",
        "df['ch-n']=dataset['PRECIPITACAO']\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "df.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "oLr57xmBAq0_"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Do total de 45239 linhas, foram separadas 75% para treinamento e 25% para teste\n",
        "nlinhas = int(np.round(dataset.shape[0] * 0.75)) # \n",
        "\n",
        "#nlinhas = 33929 # 75% do dataset\n",
        "\n",
        "max_size_train_split = int(np.round(nlinhas / 5)) \n",
        "max_size__test_split = int(np.round((dataset.shape[0] - nlinhas) / 5))\n",
        "size_split = 5\n",
        "  \n",
        "X_train = df.iloc[0:nlinhas,1: 6 + n_time_steps]\n",
        "X_test = df.iloc[nlinhas:dataset.shape[0],1: 6 + n_time_steps]\n",
        "\n",
        "y_train = df.iloc[0:nlinhas,0].values\n",
        "y_test = df.iloc[nlinhas:dataset.shape[0],0].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYAJ5BpMzkVU",
        "outputId": "654c5698-afa5-442d-86a2-71814d7117ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=TimeSeriesSplit(gap=2, max_train_size=6786, n_splits=5, test_size=2262),\n",
              "             estimator=SVR(), n_jobs=-4,\n",
              "             param_grid=[{'C': [1000, 10000, 100000],\n",
              "                          'epsilon': [0.001, 0.0001, 1e-05],\n",
              "                          'gamma': [1e-07, 1e-06, 1e-08], 'kernel': ['rbf']}],\n",
              "             verbose=20)"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "# Validador cruzado para séries temporais para 5 conjuntos de dados\n",
        "ts_cv = TimeSeriesSplit(\n",
        "    n_splits=size_split, # Number of divisions\n",
        "    max_train_size=max_size_train_split,    # maximum size of each set. of training\n",
        "    gap=2, # number of samples to exclude between each training and testing set\n",
        "    test_size=max_size__test_split, # maximum size of each set. of test.\n",
        ")\n",
        "C = [ 1000, 10000,100000] # Parâmetro de regularização\n",
        "gamma = [0.0000001, 0.000001, 0.00000001] # Coeficiente da função kernel  \n",
        "epsilon = [0.001, 0.0001, 0.00001]\n",
        "hyper_params = [{'kernel': ['rbf'],'C': C, 'gamma':gamma, 'epsilon':epsilon}]  \n",
        "\n",
        "modelo = SVR()\n",
        "grid = GridSearchCV(modelo,param_grid=hyper_params,verbose=20,n_jobs=-4,cv=ts_cv)\n",
        "                   #scoring='neg_mean_absolute_percentage_error')\n",
        "grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2uGKHy-7J3Q",
        "outputId": "d8493fc8-7f41-4107-93fd-85060336e83f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 10000, 'epsilon': 0.001, 'gamma': 1e-08, 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "source": [
        "print(grid.best_params_) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAX-Pi3Y6z_v",
        "outputId": "cde8b3c2-0d5a-4661-e6f4-9b824e186288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Melhor score: 0.6221500580024781\n",
            "Melhor Kernel: rbf\n",
            "Melhor Gamma: 1e-08\n",
            "Melhor C: 10000\n"
          ]
        }
      ],
      "source": [
        "print('Melhor score:', grid.best_score_)\n",
        "print('Melhor Kernel:',grid.best_estimator_.kernel)\n",
        "print('Melhor Gamma:',grid.best_estimator_.gamma)\n",
        "print('Melhor C:',grid.best_estimator_.C) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "TdgIaeqv9G4P"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([8344.07442274, 8543.28733645, 8474.7762612 , ..., 8684.22776145,\n",
              "       9029.64082738, 9051.94992737])"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict=grid.predict(X_test)\n",
        "predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.028547238579913"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# MAPE statistics for test data\n",
        "((np.mean(np.abs(y_test - predict) / (y_test)))) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "sTQllcxAXyNU"
      },
      "outputs": [],
      "source": [
        "#Estimate Et for treinam and test\n",
        "# For treinam data\n",
        "Lt_train = grid.predict(X_train)\n",
        "Yt_train = y_train\n",
        "#Error Et for treinam data predicted\n",
        "Et_train = Yt_train - Lt_train\n",
        "\n",
        "# For test data\n",
        "Lt_test = predict\n",
        "Yt_test = y_test\n",
        "#Error Et for treinam data predicted\n",
        "Et_test = Yt_test - Lt_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.5104174450936085"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# MAPE statistics for train data\n",
        "((np.mean(np.abs(y_train - Lt_train) / (y_train)))) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create  dataframe of Et train data\n",
        "df_Et = pd.DataFrame(Et_train, columns=['VL_Et'])\n",
        "for n_step in range(1,n_time_steps+1,1):\n",
        "    df_Et['vl-'+str(n_step)]=df_Et['VL_Et'].shift(n_step)  \n",
        "\n",
        "#drop rows with null values\n",
        "df_Et.dropna(inplace=True)\n",
        "\n",
        "Nt_train = df_Et['VL_Et']\n",
        "\n",
        "df_Et.drop(['VL_Et'],axis=1, inplace=True)\n",
        "\n",
        "Xet_train = df_Et\n",
        "yet_train = Nt_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(             vl-1        vl-2        vl-3\n",
              " 3     -115.625571  -68.513527  -35.881004\n",
              " 4      -57.641601 -115.625571  -68.513527\n",
              " 5      -50.538153  -57.641601 -115.625571\n",
              " 6      -51.330545  -50.538153  -57.641601\n",
              " 7      -69.832254  -51.330545  -50.538153\n",
              " ...           ...         ...         ...\n",
              " 33925  -48.141701   80.854656   36.359821\n",
              " 33926 -251.532834  -48.141701   80.854656\n",
              " 33927   86.711458 -251.532834  -48.141701\n",
              " 33928   64.398947   86.711458 -251.532834\n",
              " 33929  132.016809   64.398947   86.711458\n",
              " \n",
              " [33927 rows x 3 columns],\n",
              " 3        -57.641601\n",
              " 4        -50.538153\n",
              " 5        -51.330545\n",
              " 6        -69.832254\n",
              " 7        -36.464111\n",
              "             ...    \n",
              " 33925   -251.532834\n",
              " 33926     86.711458\n",
              " 33927     64.398947\n",
              " 33928    132.016809\n",
              " 33929   -280.050826\n",
              " Name: VL_Et, Length: 33927, dtype: float64)"
            ]
          },
          "execution_count": 247,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Xet_train , yet_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n"
          ]
        }
      ],
      "source": [
        "train_feature = np.array(Xet_train).reshape((Xet_train.shape[0], 1, Xet_train.shape[1])) # dados de entrada de treinamento.\n",
        "train_label = np.array(yet_train).reshape((yet_train.shape[0], 1, 1)) # dado de saída esperados no treinamento.\n",
        "\n",
        "N_Nos_Layer1=16 #Número de neurônios do layer 1   # 64\n",
        "N_Nos_Layer2=12 # Número de neurônios do layer 2  # 128\n",
        "N_Nos_Layer3=6 # Númer ode neurônios do layer 3\n",
        "\n",
        "# Resultados para o batch size encontrados, para 100 epocas, de acordo com a variação do batch size = desta forma o batch size ideal selecionado foi o 4096, considerando que a partir desse ponto o ganho é reduzido drasticamente.\n",
        "# 128 = 99s   1024 = 20s   2048 = 13s  4096 = 10s 8192 = 9.53s\n",
        "batch_size = 512\n",
        "train_data = data.Dataset.from_tensor_slices((train_feature, train_label))\n",
        "train_data = train_data.repeat().batch(batch_size, drop_remainder=True)\n",
        "EPOCAS = 200\n",
        "steps_per_epoch = len(train_feature) // batch_size \n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(layers.LSTM(N_Nos_Layer1, batch_input_shape=(batch_size, train_feature.shape[1], train_feature.shape[2]), return_sequences=True, activation='relu'))  \n",
        "model.add(layers.LSTM(N_Nos_Layer2, batch_input_shape=(N_Nos_Layer1, train_feature.shape[1], train_feature.shape[2]),  return_sequences=True, activation='relu'))  \n",
        "model.add(layers.LSTM(N_Nos_Layer3, batch_input_shape=(N_Nos_Layer2, train_feature.shape[1], train_feature.shape[2]), return_sequences=True, activation='relu'))  \n",
        "#model.add(layers.Dense(32))\n",
        "#model.add(layers.SimpleRNN(units=N_Nos_Layer1, input_shape=(1,Xet_train.shape[1]),activation=\"sigmoid\"))  \n",
        "#model.add(layers.Dense(N_Nos_Layer2, activation=\"sigmoid\"))\n",
        "#model.add(layers.Dense(N_Nos_Layer3,activation=\"sigmoid\"))\n",
        "model.add(layers.Dense(1))\n",
        "#model.add(layers.Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['mape'])\n",
        "#model.summary()\n",
        "\n",
        "#history = model.fit(train_feature, train_label, epochs=EPOCAS, batch_size=16) \n",
        "steps_per_epoch = len(train_feature) // batch_size \n",
        "history = model.fit(train_data, epochs=EPOCAS, steps_per_epoch = steps_per_epoch) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "def treinar_LSTM(X_train, y_train, N_Nos_Layer1, N_Nos_Layer2,N_Nos_Layer3, EPOCAS , batch_size = 4096):\n",
        "  train_feature = np.array(X_train).reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "  train_label = np.array(y_train).reshape((y_train.shape[0], 1, 1))\n",
        "\n",
        "  # Armazena a hora de início da execução do Treinamento\n",
        "  # batch_size = 4096 # Tamanho de cada lote de processamento usando para o modelo abstrair os padrões\n",
        "  # EPOCAS é o número de vezes que o conjunto de lotes irá ser processado pelo modelo na fase de treinamento.\n",
        "\n",
        "  train_data = data.Dataset.from_tensor_slices((train_feature, train_label))\n",
        "  train_data = train_data.repeat().batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  \n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.LSTM(N_Nos_Layer1, batch_input_shape=(batch_size, train_feature.shape[1], train_feature.shape[2]),  return_sequences=True, activation='sigmoid'))  \n",
        "  model.add(layers.LSTM(N_Nos_Layer2, batch_input_shape=(N_Nos_Layer1, train_feature.shape[1], train_feature.shape[2]), return_sequences=True, activation='sigmoid'))  \n",
        "  model.add(layers.LSTM(N_Nos_Layer3, batch_input_shape=(N_Nos_Layer1, train_feature.shape[1], train_feature.shape[2]), return_sequences=True, activation='relu'))  \n",
        "  #model.add(layers.Dense(N_Nos_Layer2, activation=\"relu\"))\n",
        "  #model.add(layers.Dense(N_Nos_Layer3, activation=\"relu\"))\n",
        "  model.add(layers.Dense(1))\n",
        "\n",
        "  #mean_squared_error\n",
        "  model.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['mape'])\n",
        "  model.summary()\n",
        "  \n",
        "  steps_per_epoch = len(train_feature) // batch_size \n",
        "  history = model.fit(train_data, epochs=EPOCAS, steps_per_epoch = steps_per_epoch) \n",
        "\n",
        "  #--------------------------------------------------------------------------------\n",
        "  val_feature = np.array(X_train).reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "  val_label = np.array(y_train).reshape((y_train.shape[0], 1, 1))\n",
        "\n",
        "\n",
        "  test_data = data.Dataset.from_tensor_slices((val_feature, val_label))\n",
        "  test_data = test_data.repeat().batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  predict = model.predict(test_data, steps=1)\n",
        "  #predict = model.predict(train_data, steps=1)\n",
        "\n",
        "  val_label=val_label[0:len(predict)]\n",
        "\n",
        "  predict = np.array(predict).reshape(batch_size,1)\n",
        "  val_label = np.array(val_label).reshape(batch_size,1)\n",
        "\n",
        "  #Calcular o MAPE (Erro médio percentual absoluto)\n",
        "  MAPE = ((np.mean(np.abs(val_label -predict) / (val_label)))) * 100\n",
        "#--------------------------------------------------------------------------------\n",
        "\n",
        "#  val_feature = np.array(X_train).reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "#  val_label = np.array(y_train).reshape((y_train.shape[0], 1, 1))\n",
        "#\n",
        "#\n",
        "#  train_data = data.Dataset.from_tensor_slices((val_feature, val_label))\n",
        "#  train_data = train_data.repeat().batch(batch_size, drop_remainder=True)\n",
        "#\n",
        "#  predict = model.predict(train_data, steps=1)\n",
        "#\n",
        "#  val_label=val_label[0:len(predict)]\n",
        "#\n",
        "#  predict = np.array(predict).reshape(batch_size,1)\n",
        "#  val_label = np.array(val_label).reshape(batch_size,1)\n",
        "#\n",
        "#  val_label=val_label[0:len(predict)]\n",
        "#\n",
        "  #Validação dos resultados\n",
        "\n",
        "  #Calcular o MAPE (Erro médio percentual absoluto)\n",
        "  #MAPE = ((np.mean(np.abs(val_label -predict) / (val_label)))) * 100 \n",
        "  \n",
        "  return MAPE, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "def previsao_LSTM(X_test, y_test, model, batch_size):\n",
        "  val_feature = np.array(X_test).reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "  val_label = np.array(y_test).reshape((y_test.shape[0], 1, 1))\n",
        "\n",
        "\n",
        "  test_data = data.Dataset.from_tensor_slices((val_feature, val_label))\n",
        "  test_data = test_data.repeat().batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  predict = model.predict(test_data, steps=1)\n",
        "  #predict = model.predict(train_data, steps=1)\n",
        "\n",
        "  val_label=val_label[0:len(predict)]\n",
        "\n",
        "  predict = np.array(predict).reshape(batch_size,1)\n",
        "  val_label = np.array(val_label).reshape(batch_size,1)\n",
        "\n",
        "  #Calcular o MAPE (Erro médio percentual absoluto)\n",
        "  MAPE = ((np.mean(np.abs(val_label -predict) / (val_label)))) * 100\n",
        "  \n",
        "  return MAPE, predict, val_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BestModel:\n",
        "  #Classe para armazenar os melhores hiperparâmetros\n",
        "  def __init__(self,  N_Nos_Layer1, N_Nos_Layer2, N_Nos_Layer3, MAPE, model = 0):\n",
        "    self.N_Nos_Layer1 = N_Nos_Layer1\n",
        "    self.N_Nos_Layer2 = N_Nos_Layer2\n",
        "    self.N_Nos_Layer3 = N_Nos_Layer3\n",
        "    self.MAPE = MAPE\n",
        "    self.model = model\n",
        "  \n",
        "  def setIndex(self, N_Nos_Layer1, N_Nos_Layer2, N_Nos_Layer3, MAPE, model):\n",
        "    self.N_Nos_Layer1 = N_Nos_Layer1\n",
        "    self.N_Nos_Layer2 = N_Nos_Layer2\n",
        "    self.N_Nos_Layer3 = N_Nos_Layer3\n",
        "    self.MAPE = MAPE\n",
        "    self.model = model\n",
        "        \n",
        "  def show(self):\n",
        "    print(\" LSTM (\", self.N_Nos_Layer1, \",\", self.N_Nos_Layer2, \",\", self.N_Nos_Layer3, \") MAPE \", self.MAPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "98J1xo_7qkgF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_94\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_112 (LSTM)             (4096, 1, 32)             4608      \n",
            "                                                                 \n",
            " lstm_113 (LSTM)             (4096, 1, 32)             8320      \n",
            "                                                                 \n",
            " lstm_114 (LSTM)             (4096, 1, 32)             8320      \n",
            "                                                                 \n",
            " dense_194 (Dense)           (4096, 1, 1)              33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,281\n",
            "Trainable params: 21,281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "8/8 [==============================] - 2s 26ms/step - loss: 100.1840 - mape: 100.1840\n",
            "Epoch 2/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.1779 - mape: 100.1779\n",
            "Epoch 3/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0441 - mape: 100.0441\n",
            "Epoch 4/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0163 - mape: 100.0163\n",
            "Epoch 5/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9985 - mape: 99.9985\n",
            "Epoch 6/200\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 100.0263 - mape: 100.0263\n",
            "Epoch 7/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0344 - mape: 100.0344\n",
            "Epoch 8/200\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 100.0129 - mape: 100.0129\n",
            "Epoch 9/200\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 100.0021 - mape: 100.0021\n",
            "Epoch 10/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0367 - mape: 100.0367\n",
            "Epoch 11/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0113 - mape: 100.0113\n",
            "Epoch 12/200\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 100.0094 - mape: 100.0094\n",
            "Epoch 13/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0066 - mape: 100.0066\n",
            "Epoch 14/200\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 100.0343 - mape: 100.0343\n",
            "Epoch 15/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0422 - mape: 100.0422\n",
            "Epoch 16/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0222 - mape: 100.0222\n",
            "Epoch 17/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0009 - mape: 100.0009\n",
            "Epoch 18/200\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 100.0226 - mape: 100.0226\n",
            "Epoch 19/200\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 100.0137 - mape: 100.0137\n",
            "Epoch 20/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 100.0177 - mape: 100.0177\n",
            "Epoch 21/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9975 - mape: 99.9975\n",
            "Epoch 22/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0084 - mape: 100.0084\n",
            "Epoch 23/200\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 100.0410 - mape: 100.0410\n",
            "Epoch 24/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0092 - mape: 100.0092\n",
            "Epoch 25/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 100.0169 - mape: 100.0169\n",
            "Epoch 26/200\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 100.0036 - mape: 100.0036\n",
            "Epoch 27/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9943 - mape: 99.9943\n",
            "Epoch 28/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9916 - mape: 99.9916\n",
            "Epoch 29/200\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 99.9827 - mape: 99.9827\n",
            "Epoch 30/200\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 99.9830 - mape: 99.9830\n",
            "Epoch 31/200\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 100.0431 - mape: 100.0431\n",
            "Epoch 32/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 100.0613 - mape: 100.0613\n",
            "Epoch 33/200\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 100.0095 - mape: 100.0095\n",
            "Epoch 34/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0071 - mape: 100.0071\n",
            "Epoch 35/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9889 - mape: 99.9889\n",
            "Epoch 36/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9998 - mape: 99.9998\n",
            "Epoch 37/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0168 - mape: 100.0168\n",
            "Epoch 38/200\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 100.0262 - mape: 100.0262\n",
            "Epoch 39/200\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 99.9889 - mape: 99.9889\n",
            "Epoch 40/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0004 - mape: 100.0004\n",
            "Epoch 41/200\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 99.9911 - mape: 99.9911\n",
            "Epoch 42/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9996 - mape: 99.9996\n",
            "Epoch 43/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9981 - mape: 99.9981\n",
            "Epoch 44/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0267 - mape: 100.0267\n",
            "Epoch 45/200\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 100.0341 - mape: 100.0341\n",
            "Epoch 46/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0398 - mape: 100.0398\n",
            "Epoch 47/200\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 100.0002 - mape: 100.0002\n",
            "Epoch 48/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9892 - mape: 99.9892\n",
            "Epoch 49/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9781 - mape: 99.9781\n",
            "Epoch 50/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9874 - mape: 99.9874\n",
            "Epoch 51/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9769 - mape: 99.9769\n",
            "Epoch 52/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9940 - mape: 99.9940\n",
            "Epoch 53/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0394 - mape: 100.0394\n",
            "Epoch 54/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0348 - mape: 100.0348\n",
            "Epoch 55/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0282 - mape: 100.0282\n",
            "Epoch 56/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0042 - mape: 100.0042\n",
            "Epoch 57/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9875 - mape: 99.9875\n",
            "Epoch 58/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9987 - mape: 99.9987\n",
            "Epoch 59/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9960 - mape: 99.9960\n",
            "Epoch 60/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0038 - mape: 100.0038\n",
            "Epoch 61/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9885 - mape: 99.9885\n",
            "Epoch 62/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9758 - mape: 99.9758\n",
            "Epoch 63/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0306 - mape: 100.0306\n",
            "Epoch 64/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0530 - mape: 100.0530\n",
            "Epoch 65/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0230 - mape: 100.0230\n",
            "Epoch 66/200\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 99.9941 - mape: 99.9941\n",
            "Epoch 67/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9816 - mape: 99.9816\n",
            "Epoch 68/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9761 - mape: 99.9761\n",
            "Epoch 69/200\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 99.9820 - mape: 99.9820\n",
            "Epoch 70/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0000 - mape: 100.0000\n",
            "Epoch 71/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0126 - mape: 100.0126\n",
            "Epoch 72/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0151 - mape: 100.0151\n",
            "Epoch 73/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 100.0266 - mape: 100.0266\n",
            "Epoch 74/200\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 100.0244 - mape: 100.0244\n",
            "Epoch 75/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9862 - mape: 99.9862\n",
            "Epoch 76/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9807 - mape: 99.9807\n",
            "Epoch 77/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9992 - mape: 99.9992\n",
            "Epoch 78/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0220 - mape: 100.0220\n",
            "Epoch 79/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0326 - mape: 100.0326\n",
            "Epoch 80/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0230 - mape: 100.0230\n",
            "Epoch 81/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9754 - mape: 99.9754\n",
            "Epoch 82/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9870 - mape: 99.9870\n",
            "Epoch 83/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9882 - mape: 99.9882\n",
            "Epoch 84/200\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 100.0205 - mape: 100.0205\n",
            "Epoch 85/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0181 - mape: 100.0181\n",
            "Epoch 86/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9887 - mape: 99.9887\n",
            "Epoch 87/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0053 - mape: 100.0053\n",
            "Epoch 88/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0001 - mape: 100.0001\n",
            "Epoch 89/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9868 - mape: 99.9868\n",
            "Epoch 90/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9884 - mape: 99.9884\n",
            "Epoch 91/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 100.0052 - mape: 100.0052\n",
            "Epoch 92/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9885 - mape: 99.9885\n",
            "Epoch 93/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9713 - mape: 99.9713\n",
            "Epoch 94/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9654 - mape: 99.9654\n",
            "Epoch 95/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9673 - mape: 99.9673\n",
            "Epoch 96/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9737 - mape: 99.9737\n",
            "Epoch 97/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9866 - mape: 99.9866\n",
            "Epoch 98/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0185 - mape: 100.0185\n",
            "Epoch 99/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9927 - mape: 99.9927\n",
            "Epoch 100/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9925 - mape: 99.9925\n",
            "Epoch 101/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9901 - mape: 99.9901\n",
            "Epoch 102/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 100.0023 - mape: 100.0023\n",
            "Epoch 103/200\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 99.9991 - mape: 99.9991\n",
            "Epoch 104/200\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 100.0016 - mape: 100.0016\n",
            "Epoch 105/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0030 - mape: 100.0030\n",
            "Epoch 106/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9986 - mape: 99.9986\n",
            "Epoch 107/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9840 - mape: 99.9840\n",
            "Epoch 108/200\n",
            "8/8 [==============================] - 0s 42ms/step - loss: 99.9910 - mape: 99.9910\n",
            "Epoch 109/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9743 - mape: 99.9743\n",
            "Epoch 110/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9692 - mape: 99.9692\n",
            "Epoch 111/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9635 - mape: 99.9635\n",
            "Epoch 112/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9714 - mape: 99.9714\n",
            "Epoch 113/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9894 - mape: 99.9894\n",
            "Epoch 114/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0434 - mape: 100.0434\n",
            "Epoch 115/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 100.0500 - mape: 100.0500\n",
            "Epoch 116/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9844 - mape: 99.9844\n",
            "Epoch 117/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9792 - mape: 99.9792\n",
            "Epoch 118/200\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 99.9838 - mape: 99.9838\n",
            "Epoch 119/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0014 - mape: 100.0014\n",
            "Epoch 120/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 100.0117 - mape: 100.0117\n",
            "Epoch 121/200\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 99.9879 - mape: 99.9879\n",
            "Epoch 122/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9882 - mape: 99.9882\n",
            "Epoch 123/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9798 - mape: 99.9798\n",
            "Epoch 124/200\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 99.9651 - mape: 99.9651\n",
            "Epoch 125/200\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 99.9560 - mape: 99.9560\n",
            "Epoch 126/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9599 - mape: 99.9599\n",
            "Epoch 127/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9699 - mape: 99.9699\n",
            "Epoch 128/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 100.0019 - mape: 100.0019\n",
            "Epoch 129/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9936 - mape: 99.9936\n",
            "Epoch 130/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9975 - mape: 99.9975\n",
            "Epoch 131/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9793 - mape: 99.9793\n",
            "Epoch 132/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9956 - mape: 99.9956\n",
            "Epoch 133/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0022 - mape: 100.0022\n",
            "Epoch 134/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9905 - mape: 99.9905\n",
            "Epoch 135/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9871 - mape: 99.9871A: 0s - loss: 100.0057 - mape: 100.\n",
            "Epoch 136/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9896 - mape: 99.9896\n",
            "Epoch 137/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9816 - mape: 99.9816\n",
            "Epoch 138/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9885 - mape: 99.9885\n",
            "Epoch 139/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9902 - mape: 99.9902\n",
            "Epoch 140/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9953 - mape: 99.9953\n",
            "Epoch 141/200\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 99.9870 - mape: 99.9870\n",
            "Epoch 142/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9941 - mape: 99.9941\n",
            "Epoch 143/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9934 - mape: 99.9934\n",
            "Epoch 144/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9939 - mape: 99.9939\n",
            "Epoch 145/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9814 - mape: 99.9814\n",
            "Epoch 146/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9906 - mape: 99.9906\n",
            "Epoch 147/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 100.0245 - mape: 100.0245\n",
            "Epoch 148/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0114 - mape: 100.0114\n",
            "Epoch 149/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9916 - mape: 99.9916\n",
            "Epoch 150/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9866 - mape: 99.9866\n",
            "Epoch 151/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9971 - mape: 99.9971\n",
            "Epoch 152/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9753 - mape: 99.9753\n",
            "Epoch 153/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9953 - mape: 99.9953\n",
            "Epoch 154/200\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 99.9844 - mape: 99.9844\n",
            "Epoch 155/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9874 - mape: 99.9874\n",
            "Epoch 156/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9718 - mape: 99.9718\n",
            "Epoch 157/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0007 - mape: 100.0007\n",
            "Epoch 158/200\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 99.9849 - mape: 99.9849\n",
            "Epoch 159/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0026 - mape: 100.0026\n",
            "Epoch 160/200\n",
            "8/8 [==============================] - 0s 40ms/step - loss: 99.9766 - mape: 99.9766\n",
            "Epoch 161/200\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 99.9961 - mape: 99.9961\n",
            "Epoch 162/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9803 - mape: 99.9803\n",
            "Epoch 163/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9919 - mape: 99.9919\n",
            "Epoch 164/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9761 - mape: 99.9761\n",
            "Epoch 165/200\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 99.9867 - mape: 99.9867\n",
            "Epoch 166/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9820 - mape: 99.9820\n",
            "Epoch 167/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9891 - mape: 99.9891\n",
            "Epoch 168/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9816 - mape: 99.9816\n",
            "Epoch 169/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9879 - mape: 99.9879\n",
            "Epoch 170/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9844 - mape: 99.9844\n",
            "Epoch 171/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9882 - mape: 99.9882\n",
            "Epoch 172/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9882 - mape: 99.9882\n",
            "Epoch 173/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 100.0081 - mape: 100.0081\n",
            "Epoch 174/200\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 99.9830 - mape: 99.9830\n",
            "Epoch 175/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9834 - mape: 99.9834\n",
            "Epoch 176/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9788 - mape: 99.9788\n",
            "Epoch 177/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9937 - mape: 99.9937\n",
            "Epoch 178/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9678 - mape: 99.9678\n",
            "Epoch 179/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9871 - mape: 99.9871\n",
            "Epoch 180/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9672 - mape: 99.9672\n",
            "Epoch 181/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9892 - mape: 99.9892\n",
            "Epoch 182/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9889 - mape: 99.9889\n",
            "Epoch 183/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9923 - mape: 99.9923\n",
            "Epoch 184/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9855 - mape: 99.9855\n",
            "Epoch 185/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9626 - mape: 99.9626\n",
            "Epoch 186/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9856 - mape: 99.9856A: 0s - loss: 100.0204 - mape: 10\n",
            "Epoch 187/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9661 - mape: 99.9661\n",
            "Epoch 188/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9922 - mape: 99.9922\n",
            "Epoch 189/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9639 - mape: 99.9639\n",
            "Epoch 190/200\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 99.9903 - mape: 99.9903\n",
            "Epoch 191/200\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 99.9710 - mape: 99.9710\n",
            "Epoch 192/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0007 - mape: 100.0007 0s - loss: 100.0312 - mape: 100.\n",
            "Epoch 193/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9790 - mape: 99.9790\n",
            "Epoch 194/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0024 - mape: 100.0024\n",
            "Epoch 195/200\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 99.9746 - mape: 99.9746\n",
            "Epoch 196/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 100.0009 - mape: 100.0009\n",
            "Epoch 197/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9751 - mape: 99.9751\n",
            "Epoch 198/200\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 99.9913 - mape: 99.9913\n",
            "Epoch 199/200\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 99.9860 - mape: 99.9860\n",
            "Epoch 200/200\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 99.9904 - mape: 99.9904\n"
          ]
        }
      ],
      "source": [
        "       \n",
        "# Efetuar o processo de Calibração da LSTM, variando os hiperparâmetros\n",
        "#N_Nos_Layer1 = 5\n",
        "#N_Nos_Layer2 = 15\n",
        "EPOCAS = 200\n",
        "batch_size = 4096\n",
        "MAPE = 100\n",
        "bestModel = BestModel(32, 32, 32, MAPE)\n",
        "arquivo='Result_LSTM_Model.csv'\n",
        "\n",
        "#n_valores_passado = 10\n",
        "##criar_arquivo_resultado(arquivo)\n",
        "#Laço para variação da Camada Oculta 1\n",
        "for N_Nos_Layer1 in range (32,30,-8):\n",
        "  #Laço para variação da Camada Oculta 2\n",
        "  for N_Nos_Layer2 in range (32,30,-8):\n",
        "    for N_Nos_Layer3 in range (32,30,-8):\n",
        "      #Chama a função de treinamento da LSTM com os hiperparâmetros selecionados para cada iteração\n",
        "      MAPE, model = treinar_LSTM(Xet_train, yet_train, N_Nos_Layer1, N_Nos_Layer2, N_Nos_Layer3, EPOCAS, batch_size)\n",
        "      #armazena o melhor modelo, com o menor MAPE\n",
        "      if(bestModel.MAPE > MAPE):\n",
        "        bestModel.setIndex(N_Nos_Layer1, N_Nos_Layer2, N_Nos_Layer3, MAPE, model)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " LSTM ( 32 , 32 , 32 ) MAPE  -3.2723802619426277\n"
          ]
        }
      ],
      "source": [
        "bestModel.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAPE= -3.2723802619426277\n"
          ]
        }
      ],
      "source": [
        "MAPE, predict, y_label = previsao_LSTM(Xet_train, yet_train, bestModel.model, batch_size)\n",
        "print(\"MAPE=\", MAPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "#MAPE to Et predicted on MLP Model\n",
        "#predict=bestModel.model.predict(X_train, y_train)  \n",
        "#((np.mean(np.abs(y_train - predict) / (y_train)))) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.5105060676558546"
            ]
          },
          "execution_count": 218,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Recomposition of Nt + Lt form train data\n",
        "Lt_train2 = Lt_train[n_time_steps:len(Lt_train)]\n",
        "Yt_train2 = Yt_train[n_time_steps:len(Yt_train)]\n",
        "Yt_train_predicted = predict + Lt_train2\n",
        "((np.mean(np.abs(Yt_train2 - Yt_train_predicted) / (Yt_train2)))) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test = Yt_train2\n",
        "predict = Yt_train_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For test data\n",
        "#Lt_test = predict\n",
        "#Yt_test = y_test\n",
        "#Et_test = Yt_test - Lt_test\n",
        "\n",
        "\n",
        "#create  dataframe of Et value\n",
        "df_Et = pd.DataFrame(Et_test, columns=['VL_Et'])\n",
        "\n",
        "for n_step in range(1,n_time_steps+1,1):\n",
        "    df_Et['vl-'+str(n_step)]=df_Et['VL_Et'].shift(n_step)  \n",
        "\n",
        "#drop rows with null values\n",
        "df_Et.dropna(inplace=True)\n",
        "\n",
        "Nt_test = df_Et['VL_Et']\n",
        "\n",
        "df_Et.drop(['VL_Et'],axis=1, inplace=True)\n",
        "\n",
        "Xet_test = df_Et\n",
        "yet_test = Nt_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAPE 3.028804213382642  MAPE_Nt_test 21.045765279025634\n"
          ]
        }
      ],
      "source": [
        "#MAPE to Et predicted on MLP Model\n",
        "\n",
        "batch_size=4096\n",
        "MAPE, predict, y_label = previsao_LSTM(Xet_test, yet_test, bestModel.model, batch_size)\n",
        "#predict=bestModel.model.predict(Xet_test, y_test)  \n",
        "MAPE_Nt_test = ((np.mean(np.abs(y_label - predict) / (y_label)))) * 100\n",
        "\n",
        "# Recomposition of Nt + Lt form test data\n",
        "Lt_test2 = Lt_test[n_time_steps:len(Lt_test)]\n",
        "Yt_test2 = Yt_test[n_time_steps:len(Yt_test)]\n",
        "Yt_test_predicted = predict + Lt_test2\n",
        "MAPE = ((np.mean(np.abs(Yt_test2 - Yt_test_predicted) / (Yt_test2)))) * 100\n",
        "print('MAPE',MAPE, ' MAPE_Nt_test',MAPE_Nt_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "1fSF564GWKfk"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnIAAAGpCAYAAAAJP9vkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABAdklEQVR4nO3de5QtZ33e+efXfS6oDwKJliBY0unGtsxEsABDm0s8y8tGWIhLECbYxmkZBZh0fOLM2A5eMbjXMh57etkOSRwYPCQdIxDTHS6DsVEMHllcvJyVMTIti6swRsbqgzRcDuIatwcQ+s0fXSXVqbOr6q3adXv3/n7WqnVOV+/e+62qt9566q23apu7CwAAAPFZGLoAAAAAaIYgBwAAECmCHAAAQKQIcgAAAJEiyAEAAETqyNAFGMpFF13kq6urQxcDAACg0q233vold784P39ug9zq6qr29vaGLgYAAEAlM9ufNJ9LqwAAAJEiyAEAAESKIAcAABApghwAAECkCHIAAACRIsgBAABEarAgZ2aXmdkHzOx2M/uEmf1cMv9hZnazmX06+ffCZL6Z2WvN7A4z+6iZPTHzXtclr/+0mV031DIBAAD0acgeuXslvdzdr5D0VEk/a2ZXSHqFpPe5++WS3pf8LEnPknR5Mm1Ier10GPwkvUrSUyQ9WdKr0vAHAAAwywYLcu7+OXf/i+T/35D0SUmXSLpG0g3Jy26Q9Pzk/9dIerMf+qCkC8zskZKeKelmd/+yu39F0s2Sru5vSQAAAIYxijFyZrYq6fsl3SLpEe7+ueRXn5f0iOT/l0j6bObP7krmFc2f9DkbZrZnZntnzpxpbwEAAAAGMHiQM7MHS/o9ST/v7l/P/s7dXZK39Vnuvu3ua+6+dvHF53xdGQAAQFQGDXJmdlSHIW7X3d+ZzP5CcslUyb9fTObfLemyzJ9fmswrmg8AADDThrxr1SS9QdIn3f3fZX51o6T0ztPrJL0rM//Fyd2rT5X0teQS7E2SrjKzC5ObHK5K5gEAAMy0IXvkflDST0t6upl9OJmeLek3Jf2omX1a0jOSnyXpPZI+I+kOSf9J0j+XJHf/sqRfl/ShZPq1ZB4AAKOxu7ur1dVVLSwsaHV1Vbu7u0MXCTPADoehzZ+1tTXf29sbuhgAgDmwu7urjY0NHRwc3D9vaWlJ29vbWl9fH7BkiIWZ3erua/n5g9/sAADArNvc3DwrxEnSwcGBNjc3ByoRZgVBDgCAjp0+fbrWfCAUQQ4AgI6dPHmy1nwgFEEOAICObW1taWlp6ax5S0tL2traGqhEmBUEOQAAOra+vq7t7W2trKzIzLSyssKNDmgFd60CAACMHHetAgAAzBiCHAAAQKQIcgAAAJEiyAEAAESKIAcAABApghwAAECkCHIAAACRIsgBAABEiiAHAAAQKYIcAABApAhyAAAAkSLIAQAARIogBwAAECmCHAAAQKQIcgAAAJEiyAEAAESKIAcAABApghwAAECkCHIAAACRIsgBAABEiiAHAIF2d3e1urqqhYUFra6uand3d+giAZhzR4YuAADEYHd3VxsbGzo4OJAk7e/va2NjQ5K0vr4+ZNEAzDF65AAgwObm5v0hLnVwcKDNzc2BSgQABDkACHL69Ola8wGgDwQ5AAhw8uTJWvMBoA8EOQAIsLW1paWlpbPmLS0taWtra6ASAQBBDgCCrK+va3t7WysrKzIzraysaHt7mxsdAAzK3H3oMgxibW3N9/b2hi4GAABAJTO71d3X8vPpkQMAAIgUQQ4AACBSBDkAAIBIEeQAAAAiRZADAACIFEEOAAAgUgQ5AACASBHkAAAAIkWQAwAAiBRBDgAAIFIEOQAAgEgR5AAAACJFkAMAAIgUQQ4AACBSBDkAAIBIEeQAAAAiRZADAACIFEEOAAAgUgQ5AACASA0a5MzsejP7opl9PDPvYWZ2s5l9Ovn3wmS+mdlrzewOM/uomT0x8zfXJa//tJldN8SyAGjP7u6uVldXtbCwoNXVVe3u7g5dJESKuoRZN3SP3JskXZ2b9wpJ73P3yyW9L/lZkp4l6fJk2pD0eukw+El6laSnSHqypFel4Q9AfHZ3d7WxsaH9/X25u/b397WxscEBGLVRl+JHEK9m7j5sAcxWJf2huz82+flTkn7Y3T9nZo+U9Cfu/mgz+4/J/9+SfV06ufs/S+af9boia2trvre319FSAWhqdXVV+/v758xfWVnRnXfe2X+BEC3qUtzSIH5wcHD/vKWlJW1vb2t9fX3Akg3DzG5197X8/KF75CZ5hLt/Lvn/5yU9Ivn/JZI+m3ndXcm8ovnnMLMNM9szs70zZ860W2oArTh9+nSt+UAR6lLcNjc3zwpxknRwcKDNzc2BSjROYwxy9/PD7sLWugzdfdvd19x97eKLL27rbQG06OTJk7XmA0WoS3EjiIcZY5D7QnJJVcm/X0zm3y3psszrLk3mFc0HEKGtrS0tLS2dNW9paUlbW1sDlQixoi7FjSAeZoxB7kZJ6Z2n10l6V2b+i5O7V58q6WvJJdibJF1lZhcmNzlclcwDEKH19XVtb29rZWVFZqaVlZW5HROD6VCX4kYQDzPozQ5m9hYd3qxwkaQv6PDu0z+Q9HZJJyXtS/oJd/+ymZmk1+nwLtcDSS9x973kfV4q6ZeTt91y9zdWfTY3OwAAMG67u7va3NzU6dOndfLkSW1tbc1tEC+62WHwu1aHQpADAACxiOmuVQAAAAQgyAEAAESKIAcAABApghwAADOEr7WaL0eGLgAAAGhH/mut0u+XlTS3d3vOOnrkgBnFWTkwf/haq/lDjxwwgzgrB+YTX2s1f+iRAwbQdW8ZZ+XAfOJrreYPQQ7oWdpbtr+/L3e/v7eszTDHWTkwjKGHNPC1VvOHIAf0rI/eMs7KgX7t7u7qoosu0rXXXtvpSVoVvl92/vAVXUDPFhYWNGm/MzPdd999rXxGfoycdHhWToMOtG/S/pa1srKiO++8s99CYebwFV3ASDTpLat7uabPs/KhLyUBQ5vUy57FkAZ0ibtWgZ5tbW1N7C0rGsPS9A7U9fX1znvfuDsW8253d1f7+/ulr2FIA7rEpVVgALu7u9rc3NTp06d18uRJbW1tFQaf1dXViQeKMVyuGXPZgK5VXVKVGNKA9hRdWiXIoZY6AQTt6GNMXVNjLhvQtaITmdTy8rJe85rX0EaiFYyRw9T6eGwGzjXmO1DHXDaga2Vj33Z2dvSlL32JEIfOEeQQjIfMDmPMz4Uac9nmFTef9KfohGVlZYUAh94Q5BCMh8wOY8zPhRpz2cag71BFr3m/pj2RiTl091X2mNdRb9x9LqcnPelJjnpWVlZc0jnTysrK0EUDRmdnZ8eXlpbO2leWlpZ8Z2ens89kH+3fzs6Or6ysuJn5yspK8PYdon60pa+yx7yOuiBpzyfkGW52QDAeMguEKxoIv7i4qPvuu6+Tm4W4+SQeRfVjeXlZX/rSlwYoUbi+7lbnrvizcbMDpsZltGJ0/yOvaMjBd77znc4ue3LzSTyK6sc999xTWSeGbm/6GmbDcJ5Ak7rp5mHi0mr8ml7S6KIc89T9P5b13paulqfoMmd+avOy57zVxZiV1Y+yOjGGbdzXJXyGCpxNBZdWBw9UQ00EubiNoTFLzVNjM6b13oZJyyPJl5eXp16movfOT2bW0tI88LmzFLSH0Mc63NnZaVQnxtDeMEZuGAS5AYMcDWv7xtCYpcys0QE6xnoxpvXehrJekTYOGNltvLi4OFPrLlZV+12f4WF5ebl2nWja3rStr/YrxnayKwS5gYIcZxTdGEtj5t4s3MRaL8a03ttQtDxdhKxYt3lW7AfVkG3Q58lKkzoR88lU7PVnaAS5gYJczDvdmI1pvbbZGC8vL/dY8kN1Gtcxrfc2VI1ji/GyZ0iPU5MyzEIQDam/fZ+s1N0esW6HWMs9JgS5gYLcrPVg5A11hjW2RqHueijrCSr62y7Wdd31OLb1Pq2qcWzpAb7Let7me1dtn2m23yyE+JD2OIbljLFnK4b1OnYEuYGC3Ngr7zQNwtAH9Rgbs1RZT9CkutHVum56WTjW9T7Jzs7OxLFK6frtsp63/d5V23Oa9mgWTkpDln/odm1WzUL9GRpBbqAgN+ZGYdqylV0enLWDfdvq3rFWtK4XFxenWrc0rg8oqrNdnoxVBfq627Zqe06zvcd+UhoipM3LB/s27mDGbNSfoRHkBgpy7uPtwZh2xyq7PHjs2LFRhtdptL0d69yxVrauu+zBQf3wU6eeVN1sUXfbdtkjN+aT0jrKts+sLOMYsW6nR5AbMMi1oYswOG1vTFlvQtfhoKsB3WWf13YjVOc9q9Z103VL41qtTvipuz5D9qE627bLMXLp34/xpLQtnNh0a9brT9cIchEHuTGNj8qXq06Qa+tyXdcHq0naaOAnNWKhDVvVoPxp1i2Na7k2AndRPanark22bd8nObOEoQYYM4JcxEGu6OAw7aMq2gg8RZcHuzyr7fLyUZFpG/g21vXOzg4PlW1o2vAS+vdN6kn63l3vN6g2Sz1ysdxpHePnD4UgF3GQa/KoilBtHODyAeXo0aOdjpHrckB3ftnSdTNtgGrrADG2S6FjbFDzZTp16lRv62ya7Ty2bVtljNu+yDQ932PeBkViutM6ts8fEkEukiA3qcEZ+9n6NJcMm+ijRy7kkledxqPNSzZjOYB23aA2uXtwUpmK1n0X+87YxqB1VVdiOpjWLWsb62zofbTLnsWhey2H/vwhEeQiCHJFDc6VV15ZGCbauHNubELG+HQ9Rq6osVhcXCxdp0VlH0vj0+b4qS6XaWdnx48ePXrOex87dqxRmdoK0aFl73vfKzqZ6ipshW77MbRDfe97Ywi5XY71m2b4QBv1YJ7HMRLkIghyRQ1O2aXVSY1RWa/ESnKpKbtT5X8ecnB00aXa5eXlWj1+05a5aWNV1ICPoXFvOwA3bVBDtk1ZICsLC6EhbogQPUlbvT+TtluTL2QPFbLtu6rzdddZ3wf+MZy01S1DlydwbdeDMazfoRDkRhLksjvM8vLyWQGlzkGobGdo8l5tHdynFVL2PgJQk4NgVQMzdO9EVfnqNpBNGtTQ+lQWyqrCQtHf9F2HqoSui6p6U3d/T9ffNPUxZNt3ccBt0h71feAfQ49RnfXU5NJznfdu+watMZwUD0UEueGDXNVBp26PQt2ekjpT04P7tELLXtYrM21IKrusd+rUqXNCXjp2q+0GvK1lqhpnWXWTSFFd6/KgWhVO0vURGvzr9kI3Xcd13i9kXYSs4yY9kaE930XLN+kGEjPzU6dOVZZrmkDT1slDvqxt6irA1q1foX/TdJ1WvXfI8a6poU+KhyKC3PBBLuSg0zTI1P2ckHK4j+eyRNnn99V1f+LEiYkBTzoMeUW9eE0eE9PWMoX0WFWF9rLPrtOg7uyEfy1ZUZjOlymknuQP2G3Xl6bvF7JvTdPztby8XFiukP0sDXaT2qV07G5ZT2cXgSa0/uSdOnWq9V7Zoro/lvoVqo02Plun0h64op640Howr2GtjAhywwe50JBW9Wy2qp049HJTyE7Wd49c2cG+7oGsaRmb9mguLy83GqA/SVvLVHXArrqMPu36zDbwTcZ6Vu0LVQeL9L1DwkvT5zI23VYhfzftWLSig2EbvfZVl8za7gkr6/VuY11XfXZVj2R+X2oaQvJ/W3eYR2hvWfqa0EufdYJr1VTVJs7z5dMyIsgNH+RCzoJXVlZKXzdpx5y0g4UeQCdNQ46Rc69+yHD+ElDR65r2Gjbt0UzHPU5zwEi11RNaFZ4m1aXQ9Vl1wKjTwFcd3MuWI+QzQsvRpF5XreOiS5WT6kp+3woNIGUH2Unzm9bx0P0g1WZPWFGZzayV3s8iRYG0jf085LNC98eiv8+v75DPCPmb9DV1HgqfTlUnTX13IBQZW6+gCHLDB7mqHSjdMeo0OqE7brYyPvjBDy498IQExdDf1730VhQ+05+Xl5fPedhw0XTixIlGO2HTHs30s+ocMOoeaKsastCz+eXl5cJ1E/LZIfWuTlioWq6yMlX1toX02tU9UIT08NWtww9+8IODwnBoECrrEWuj135hYaGybk070D1bn8vKkn9tkzpdpE49nnbIyTT7zM5O2I0FRZ+RPlopfxNeWV1vEuKy66qoXY7thpG+iCA3fJBzP7uxmbTDuNdrdKp2ykk7StVOUjd81XnkRtGA6qoDS3pDwTQ9CVWXmyZtp9D3Ti8V1Nl2RevnxIkTpeWvsy0mfctG1TdvTBPSsvWubqNepmlPQ5OwUiUkBDW5dFnUs9S0V6BoG6SfU3bgD5kWFxeD6laT7R26nsvWefYybp22KK/pzSR11G1v8nV/Us9n0foua//b2ofqTJPatWmCd+i67uImkK6JIDeOIDdJdidOG9ayRikrpJGpc7mmqsHLh8+qHp+QHTl9z5DlmLbROHHixMR1dvz48bPKU+cS1PLy8v13QU7adm08JibkWw3KDtwh6zrf41bW2IXUuzoHwMXFxcrlqzv2J6S3blKZm67n7LI0rZ91tkGVkDGJZWUJ7flOe+bK2oOQfaeLy7/Z7Zk/iQ79GsEm5ajTc1M3sObbgrKrOJPqVdnyFNXdpnU6NNjnw1Fob1j+0m5VO1nnUv8YegXzRJDrP8iFHnhCd+I6Zy6TdsSQgbptNJ5dTtMcJOt+TtVBKW18ysbQtHWmP6mhy9etkPcra1TrNFBd1JOqZ1c1DTWh66ZoXedVvVeTHsn852fr0KTfhyx/WRnSgFP0GXV7wNMe6TZuoqgaYtJkfYaum0lDDqYZahGibl3JX22paqfyJydNl6dpz2TI5fGiYUNl+/zOTr1vfymrU3XqCD1yI5q6DnKhZxR1d+JsL4OZFV6Gq2ooi86C22g4u57aauDbKEfZNsyH52l6iiSV1q06U9nYptCz2yZlSN9vZ2ensAzpepPOPhBMO1alyXYtUrXd0nJ3ebAMuUOyKqiVLUd6MKyzHG2eZIWcSNWZ8uuq7vZYXl4urbNldaHqBpRplqvqET3pNGn/rfPZdddZ3V7WJuGoqkyhV4/ydSS7jhgjN/Kp6yAXenBvsvO2cYlx0l1DbZ4FdzWNqccwbXyarLM0TDe5pNLGOsj3zC0uLhYeiIvuKM0ekEIO4tkxinXWU9Xz+crO3EN6LPJT2aDv9D2rDqDZ8C51d/Jx5ZVXlh5sir6n+ciRI0H1e8j9LXR8Z2g9qru/TTtNuoTXdxnS7Vi0/4YuR7Yuh67v/M9tnZSl2t6nii7dpm1L9kQ3be+K2oiuiCDXb5DrquFu86w3f8BrcsbZ55Su07J10Nel1+y4waafme9dXV5ernyvtg4CoWfy6XrPXyLJB5vQsNR2OJjUI509aNRZzpD1kIbakOXN3iEacvmrrXKm69m92XiobJnGcGKXHYOXrYd111tfbUNZufosg1Tva7mqlmfSA6DrLOOkE69JQWjSmPFJIamrk4x8OYrGVNZZ320RQa7fINdFJWv7TC4dYB5DiKuzA3V18FlYWKi1U1dN+Ut4oYFo2oNBk21d9C0BQ/QwdDWFrte6PddVvV9lvY5Np7RuVZWzatv1HTyKpknjniYNdC/qgexzantbNp3KglHZdp2m/axqF6cZq5e/o//EiROd1c+mPb/peu+KCHL9BrlpxzGlFbXODthkWlpaajTObgxT0SNWujpTq3rESJPyZ3u12g5ERd800fY2qPP6MfTwTJqGuoSY1t0u1kvVuKD0s8cSPEKm7CWt/IHWzEqfkdnXyWqfl6SrHiVU9ViStqeqJxBkhy3E3nlQNHV5V6sIcv0GOff6Y4iKKsXQFXOsU9EO02Xj1UWQrhsOQxrA7GD1sRyoqctnT9kz964O/EePHi1c79kHEO/s7ER7Qhc6TXruXRdT0fPYuqj/V155ZaObXZi6m+iRm6Egl9+5xtDl38eU9jL1cbltYWHBT506dVZQOXHiRC+Ndds7ftsNbt1n4R07dsxPnTpV+2AeGmzHHhKGOOBlT0Sq9pkrr7yycRtSFfzTujKWy6hdTkXPkWxzSg/kp06d6uxKSn6aNJZsTDeGzdPEGLkZCXJ9BZmxTu79d+mPaaq73OmlhrbLUed5XAsLC2edyYd+xiyMkRuqnubP3KvCVMidptPUlaG3w5impvtk9sagodZp28/hY6o3dUmzHuQkXS3pU5LukPSKqtd3GeTm+UyIxqP+1OVYtjrjdbJja0J6ENIepTFdvg1Z1/lnXA1RjqKHlnax/8zqWKQupyZ37Gbvah76GNDXOL1p79yfxalLKghyRzQDzGxR0u9I+lFJd0n6kJnd6O63D1Ge06dPS7pKGnz1mo4cOaJ77/12b594mKnnk9mC3O+r9TfHjx/X3/7tgr797b/rpEz7+9Lx4w+S9JjK1x4cSL/wC+/VBRes67GP/SV95CMfLn29u/Tud0u33fZd+u///YckfbOVMmc1Wadl7rlH2t6+W0tLT5P7Y3VwcLHOP///0ze+8fXWPiPv/PMfIkn3f8b55z9EGxsbuuCCH9G73332ay+66DqdOfPFVj//6NEHSXJ985ttbx/T4bFrKPU+//jx4zp27HjQtr7ooodLUq1t8S//5S9Kkh7+8JfozJnHKGSf68r+vvT4xz9B+/sfUVfb6PzzH6IbbniLJOm5z31u5ecsLCzqvvu+00lZxuE2LS9/a5BPNp+BI6+ZPU3Sr7r7M5OfXylJ7v4bRX+ztrbme3t7nZRndXVV+/sflnRBJ+8PAADGY3HxH+uGG56j9fX1zj7DzG5197X8/KG7jNpyiaTPZn6+S9JT8i8ysw1JG5J08uTJzgqztbWla6/9Y0knav7ltGe4h39/3nnn6fu+79G65JJL7v/NBz7wfv3d33XT6/PsZz9H73nPu6tfmFhcXNR3vnPumZmZSZJm4eSifYfb9ujRY7r33ntb7aUau8c//gn6q7/6VFJ/q/eRtP5LquxVPFv5ez/+8U+4f5+6++67K987vx/efffd9y/H0aPHJEnf/va3znndLbfconvu+VKNcpc777zz9CM/8nTdfffduv322/Xtb0/Xa3Deeefp4osfrjNnvhjUppw8uaILL7xQH/vYxwbtkUnXQ1VbdfToMV1xxRWZOlft5MmV4PUxC8xMl112UnfdddeM97KFOe+88/Tyl/8Tra9fNUwBJl1vjW2S9EJJv5v5+aclva7sb7q+a7XNMUPpWI0mXzpcVY5p7/JM71INHY+Rvz2+6utPJn1FT9M7INPxUXUHIWfvPHvQgx7U21iLoudDtfksu7FP+RsCqr7Wru6T2EOn/P4Vsv6zX21WNfg9/0T4tsqdrTNtrI90/62zD2XXQch662qcbdV3I+fXW9310kW5s98OMrZ9ft7HxWWfA9oXzfLNDpKeJummzM+vlPTKsr/p4/EjfVaqugc86ewvMZ+mkUif8B86YD9t8LJPHp/0zKVsA5b9ftqi5zOVNaTZ9VPnUQtD3bxR9LnZ5RiiXH1P+e95LTsx6XJb5csR8lnpHYzuYeEh/SL7tgaPZx9B08aJZZNnHqbrIbuvV73++PHjnT0guc6NOXVuEmnjxpmiz8vu813cVFT1EN+xTMvLy6UPfB5iyj+Mvmua8SB3RNJnJD1K0jFJH5H0mLK/6frxI0OcPWUbyzoNYbbBb3oHYvrZde+QK/t6ouyBMFW0XqvOiPM9KqEH4qEbirLlGNsZcfabNvLP92s65QN438u0sLDgV1555TkPXK3TAx1a39qc0u+1TLXxfum+2PQ96nwVWVqX2lofXX+dXJOeyuxU1lua3efbLHO+fe36kVHTfO3VmKd8x0SXNMtB7nD59GxJfyXpryVtVr2+qyA39DPkmn5vY/7STpOz7i6WP9/TWNTQVJ0R599nbJcpmqyPocuSn/Jffp3dJ/KX0EN7nbIHsSG22bTfMVvncl5b06RHm4Qua1rOSftZOjShr1Da9LEpi4uLU4XvsunEiRMT60N+uEhajknr8ujRo+fU/aNHjxa2213s89lHpbh3H+KynQz5Kywx9AaGTvnjaNs060Gu7tRVkOujwU7PVtvukcmexdfdqfM9J21eIsqGgLLLDzs7OxMv72YPbNmGtmoZi8qePeDln9ze9tisskaiy7qWrvOm4xEnHdzyIS+kjqXjUIbojataPyG91+l+0eYJTr4+5784ftKBpKqcaZCo2meb7jN9TPleyLw2gkraizWpPk8yafsUbYuik4YmD+qu2kb5E62uQlzR8xLb3iemmdruKcx3GrRJBLl+glzXZ6vZHbuLz2ry9S5lZyHT7qwhy5hv7IoObE2+/zD/+5Azrjpn/3UOfPkz2qLQWBR2Q7+eaHFx8ZxlahIaiy431R1r1bShDQkn2VBet15mt/ekwJuvK/ltF/I5+QN/UVCrUnSSk9aLNg5kIfWlTptVNxQW3fBV96SyLGzVOUgX1f2y8peFxKp6GnISOan9mvaEcFIAlc7+Lt9J2jwRzY4vnXTjXNr7lz8Bz/cUTqqj6c9lHQlV9bAtIsj1E+S67CXJN+JFn1W0Y4VM2R6EqteFnJGm7xXaCzZpByqbsl8t1XTbVDXwTcdAlNWFul+jUxSKsmEl2yhNOoDUuYSR1+Skoexyd9HytDVNurxb1dtRZ51NOqDX6ampqh/58rWhqAexjRPC/PqYdtum3/1bZ5tkD+bpgbtOGbKXG0PqS9W2rtveVIXEsu2UXe/5dVBVzmm2fzrMoMnJRlsdEWW9fk1M2rZ16jM9cjMQ5Jp0U1cdZIt2wLLGJqTnpqgxS017Vlq0Q2TnFX1GnbPxUGXbpWonDVnmkDts8w1daPAv68EKDRV1QmNekxOUNGCW1bN8Oet+RlHdKdpfyg5qoQ14WwGrrIe47klDqC5ONIvWR0hvWLqcZb3oIdtkUq9tUd1Lh6Zkx2uG1oei7TZpHdRpbya12/ky1dn/Q4T2iueXvaqNC90/2qiLdU7kpxFaVsbIzUiQc/fKjV2nFyW0t6uqFyD/uqLbuLONwjQHsdC/bePSW6iyhrDq86q6y4uWo+qMOORMLx37UxWKmi5/yPZt0sNy7Nix0t6TpmXsu0HN1o38JZm23ju0F29abfaI1ynztHXXvfimmbITwqaf2UagCmlv6hwD2jypqNqf8zdDFJkmXIYG8qKbi7oOTVll+0Kf+68Icv0FubKDUZ2w1WWlmDRm5ujRo8FnpWXvW7X8RX8TcuYbGgiKytZ0MPGJEydK18O0DVrR35fdzVb0GWXrs+p9yrZvnXCd3T5VN6BUbaOmU9OeiqJlb/tA0neIcw8fu5YuWxtjxco+N/R9qtZ/k5uzytrhJp+VD4h160zIOmqrzjQ9RuVNG9BDrtqU9c72Zdr62xYR5PoLcl1ejqlbjqKKv7Mz+UnvIWdhZZ9XdRCuOxC0KuSE9g4WncmHnGlPmvLbs40GbdK6O3LkSK1yVNW9sgNzaENZZz2VDewvagSz5ag6GJe9ps1Bx2035EO1EXV7jtsq57TvU7X+QwNqyOc3/aw6J6mTVNXl0PAS8plt9JDWXRcxG8sxXQS5/oKc+7BnD+nnl1W8soavi7O8NnbwfA9ByKDaOjtgk7P6qmUPXd66PV3S5LFgVeUoO5B3sZ7KwlbIQSNkvTZZ93X3z7YOfNOUuS11lr3JPtfG5+aFjLUsC6h12qMmn9XG+glpA6rCQ2h711b9G0vA6cPQx3R3glzvQW5oVTtq1TX/UKG9J1U7eEgXe5NxEnUarLphKnsQn7ZBa3oHV379hJY1v67r9JqFrqeqsY7ZcFnUQIas16KxNmUPKO6yHhUpW8+TtlPoe057cKkztCEkSHRxsAtZ/2WfXSeIh35W0bZsehdl6LCCsjoXWk/r9sxWlbvtbT6G0DRGIsjNV5CrarjKDsbTXg4sakjqjEmpcxdaWcNWpwHf2Sl+1lZo41jW+JT9vm6ILFsfddfRzk7x2Lmi9dTW5bnQoFbVqOcvn08aNpD2ljQJZdMG9dC6VTcYTtsbUvYedddTl70zXV+aDf2sbD0ru7O+6Reqh5wYl7XPddu7aU+U6y5TSCibp16+ukSQG1eQKzsLbuNMJKQXpEkwCvmMujtgkxAT2rA1ORjlLyW10cBVNU5FYXaa9ZJOVT0EZeu/bD3VbZwnvT6knuYPNlWfW7Y8VXfqNVmGEG3tKyHvmZYtJPyWPRak7uXkJgG5jmnq3KRw3+QKQZ0bcaYNHyHrM1/OaW5O6WL7DdX7PatEkBtPkCvr0ejy9vL8e036fr22Lge28WiC0KnLnpTs+0wTsJs0ymU9ZXWmqptCytZ/nV7FJsrCQsiBc9K2rKpPZd/yEKLJOgi5caPuuqwayD9pPWV72kL23zr7W9vjCKdR93J7iCYnnF2GoNArGaHtXRfbr0koG1M9GhsR5MYT5Ioqd9Mnfhepe0mqrV6GuuWt00A2CZ5djrcIfe+mjVPdutLkgFL0Gflg08Ulj7I6FFov8ssW8nd1HolSdx1MqhNlZWq6f9etG3W+8SUtdxuPz2h6mXEaXfTqND3hnGbZmwzHyH/DRReXn0M1affokSsmgtx4glzdBmGsZyJt9naFnlk2GYzblTrL37RxqtN727QOhS5HFw1s2WeH7if5ZavqyVupeQkqZFxU+nd1L7+lX0XVpE4XrbsmYaNo25cFiZAxVtO0C9Pos3epqp52textL2PfJ2p1yjFtb+qsEEFuPEGurx65PrTV21U0JqWr925DnUZqmkYydDzlqVOnGteh7N8uLi5OfJ5gV5c86o6fC13fk8JaVUisGwrzfxda5nSa9IX1dW+iyK+7Ot9yMKkNCq2TRScYY2jH+j7pCDl5aFtXy9hmW9m03ZtmfOMsI8iNKMj1MUYOh7o4y0zVDTaTglfbAbPJ8g7ZI1d3WYqCWfr60JOB0GWpGyZDehFDXjPNOi0KcgsLC8HrskrZ+hvDGKeu9vuqHsqybd62Ltu2NvUxvnheEOQGCnKhvSkhlzJQbNoDdhPTvHfTwBVSN+rWodDlGOLAERp+65atLCRmH+oaErqyn1O3R66LA3+dmyDS19Zta8rC2lgOvE3a0i5DRxdt+zwcL8ZwYjAWBLkBglwsZ0x96LLBaTLOqo1GYJowVnSgLTrYjaVncawHjibhoewyZHrjQ9H7Li4uFvbKTHsXdhomu1gXbW2/qs+Isd1ro9xcbWnfWE4MxoAgN0CQowJWj1VqQ9l67nob1DkwhlwqHOIZXbNQT9u+K3iaUFL2DRuhU9NvB3Dv5wSy6jPGGvjLtLUfTFr2WdjHhhLriUEXCHIDBLmuu4TH3lj2NQC47rPIujiohWyHkMtuRetkbD2LY9P0QFn1LDX3ZvtZUU/fiRMnaj0CpI78APGu7/Ibe/tTV5f72KxdHux72w9Z18ZUzwlyAwS5Ls/CYjj4VgWXthqxqvU81GXdvGkeUzCmnsUxaro/VPXINVV1cpFd123sHzG0B2NHr3eYOnVtXtuVrhDkBghybVaC/A4xzVex9KUquLQZQoba2eo00CGX8YqMrUFpoutGveng9qYPBy7TRr2os3/MUlAYSpf72Czsv6nQuha6zGMOe2PbrxoHOUkvkPRpSV+T9HVJ35D09aq/G/s09F2rdf6+znOhxtRVXxZchrq82ba6NwpM05jXXcYxNZBjPpDl97HsXavTvGedXotp182sXbobSte992PZH6cRWtdCQtCY2wX38e1X0wS5OyT9/arXxTYN+Ry5UCGD4/s6U2ja2zGp/G0cKMei7hlbX4352BrIsZ3Z9qHujTDT1It5XL8YRmhdCwlBY6+3YyvfNEHuv1W9JsYphiBX1qPVRy9XalIoCP3KlFk5Cy0ytsCUGlsDNO3Dk4den2M31noYI+peudC6FtIGja3HK29s+1XtIKfDS6ovkPQaSW+T9FOZeS8o+rtYphiCXNUYs6ZfjlxXSKCc54NGnw1/6GeNrYGsEyzH1njGIl83xvS9xEML3W+6qnuzFg5DlidkXY7thHOSMW27JkHujSXT9UV/F8sUQ5ArC1B9HthCH3A6pp1vFtU5yIytgYy57DEiDD9g6Lo3a9uizSEDs7ZuutYkyP1jSctFv499iiHIjWWMWegl3r57e8Z0ptSH2Hu1Yu1NjBFh+AF11kUXda+s/Yyt3eqiXZm3dnwaTYLcL0l6v6T/KulXJT1FkhW9PrYphiDnPo5KHnrTRZ8HiTEGlTqabNd5GWc2jyGk7W1FGH5AnXXRRd2b5vmRYzOP++aY1A5y979AOl/Sj0n6j5Juk/SfJb1Y0iOq/nbMUyxBbiyyB5rl5WU/duzYoI1RzA1K0xAa8zLXEXtIr6uL5Z2XuhJi6J7skCsasWwXThCG1TjInfMH0hWSXi7pprp/O6aJIDedoXt7Ym5Qmh5k5yngDF2/+sS4rG7VXRdt172QKxoxtFvu09XVedqnu1I7yEl6YtlU9HexTAS5uMXc4zBNCKUxnD1dnZRQVx7Q9boIGdRfNVYuBk1PEDixaEeTIPeBkun9RX8Xy0SQi1vMDUPMIRTtoz7ErU5bFHO7lWoSiqnj7Wjt0uqsTAS5+MXa4zALjTnaQ32IW92QEmu7NY2Yh8KMSVGQs8PfzZ+1tTXf29sbuhiYU7u7u9rc3NTp06d18uRJbW1taX19fehiYSDUh3gtLCxo0nHUzHTfffcNUKLxWV1d1f7+/jnzV1ZWdOedd/ZfoEiZ2a3uvnbOfIIcAADNEFKq7e7uamNjQwcHB/fPW1pa0vb2NicsNRQFuYUhCgMAwCzY2trS0tLSWfOWlpa0tbU1UInGZ319Xdvb21pZWZGZaWVlhRDXoqAgZ2bPM7N/k0z/sOtCAcDu7q5WV1e1sLCg1dVV7e7uDl0k4ByElDDr6+u68847dd999+nOO+9k/bSo8tKqmf2GpCdLSlvRn5L0IXf/5Y7L1ikurQLjxaUYADjbNJdWnyPpR939ene/XtLVkp7bdgEBILW5uXlWiJOkg4MDbW5uDlSiQ/QSAhib0DFyF2T+/9AOygEA9zt9+nSt+X1Iewn39/fl7trf39fGxsbchDlCLDBOIUHuNyTdZmZvMrMbJN0qiVGcADpz8uTJWvP7MNZewj7Me4gFxizo8SNm9khJP5D8+Ofu/vlOS9UDxsgB4zXGMXLz/LwwHrEBDG/ax49cnPx7RNI/MLMXtFYyAMgZ452AY+wl7MsYL3UDOHSk6gVmdr2kx0n6hKT0tNMlvbPDcgGYc+vr66O6Q3Vra2tiL+E8PC/s5MmTE3vk5iHEAmMX0iP3VHdfc/fr3P0lyfTSzksGYPTmaQD8GHsJ+8JDb4HxCnmO3Bsk/Vt3v72fIvWDMXLAdMY4jg3d4ftggWFNM0buzZL+zMw+ZWYfNbOPmdlH2y8ixm6eel9QbZ7v4pxHPJk/HrTV86VyjJykN0j6aUkf0wNj5DBn8r0v6eMHJNGgzykGwAPjQ1s9f0J65M64+43u/jfuvp9OnZcMo0LvC/Lm6S5OejgQi9jaavat6YX0yN1mZv9Z0n+R9M10prtz1+ocofcFefNyFyc9HIhJTG01+1Y7Qm52eOOE2R77navc7FAPDwTFJPMwAJ66j5jEVF9jKusYNL7ZIfPIkZfw+JH5xeMHuAQwyTwMgI+phwOIqa1m32pHZZAzszea2fX5qY/CYTzm+RlaEt81Oc/maSwgpjOGk72Y2mr2rZa4e+kk6R9lpnVJ75D02qq/q3jPH9cD3xSxlvvdKyXdIelTkp6ZmX91Mu8OSa/IzH+UpFuS+W+TdCykDE960pMcCLWysuI6/EaTs6aVlZWhi4aO7ezs+NLS0lnbfWlpyXd2doYuGgLs7Oz4ysqKm5mvrKx0tt2q6klf5YgJ+1Y9kvZ8UqaaNLNs0mEv3v9T9+9y7/H3JT1a0p9kg5ykKyR9RNLxJKD9taTFZPprSd8t6VjymiuSv3m7pBcl//8Pkk6FlIEghzrMbGKQM7Ohi4YecBCOU59Boexkj8BSjH0rXFGQq7zZIc/MHi3p3e7+vbX+cPJ7/YmkX3T3veTnV0qSu/9G8vNNkn41efmvuvszs6+T9JuSzkj6e+5+r5k9Lfu6MtzsgDoYlAvEp8/9dmFhQZOOp2ZW+F21tB+oo/HNDmb2DTP7ejrp8DEkv9RFISVdIumzmZ/vSuYVzV+W9FV3vzc3fyIz2zCzPTPbO3PmTKsFx2yLaQAxgEN9DqYvG+/FoH50KeSu1fPd/SGZ6fvc/feq/s7M3mtmH58wXdNO0etz9213X3P3tYsvvnioYiBCMQ0gBnCoz8H0ZSd7DOpHlyofCGxmPyjpw+7+t2Z2raQnSnqNV3y7g7s/o0F57pZ0WebnS5N5Kph/j6QLzOxI0iuXfT3QqvX1dYIbEJE+H1qdtg1Fz1Wch4dnYxghX9H1ekkHZvZ4SS/X4U0Hb+6oPDdKepGZHTezR0m6XNKfS/qQpMvN7FFmdkzSiyTdmAz++4CkFyZ/f52kd3VUNgBARPruSS96riI9+uhSyDc7/IW7P9HMfkXS3e7+hnRe4w81+zFJ/7ukiyV9VYc9fumNDJuSXirpXkk/7+5/lMx/tqR/r8M7WK93961k/ndLequkh0m6TdK17v5NVeBmBwAAEIvGNztI+kZyl+i1kt5tZguSjk5TGHf/fXe/1N2Pu/sjsneZuvuWu3+Puz86DXHJ/Pck4/O+Jw1xyfzPuPuT3f173f3HQ0IcAAB9GcODgjG7QoLcT0r6pqSXufvndTgO7dWdlgoAgBlQ9a0whDxMq/Zz5GYFl1YBAF0re5Zd0c0YjJ/DJEWXVkPGyL1A0m9JergkSyZ394d0UdC+EOQAAF3jQcFoyzRj5P61pOe5+0OT58idH3uIAwCgDzwoGF0LCXJfcPdPdl4SAABmDA8KRtcqHwgsac/M3ibpD3R404Mkyd3f2VWhAACYBTwoGF0LGSP3xgmz3d1f2k2R+sEYOQDA0HZ3dwtDHpDV+GaHWUWQAwAAsWh8s4OZXWpmv29mX0ym3zOzS7spJgAAAEKF3OzwRh1+B+p3JdN/SeYBAABgQCFB7mJ3f6O735tMb9Lhd6QCAABgQCFB7h4zu9bMFpPpWkn3dF0wAAAAlAsJci+V9BOSPi/pc5JeKOklXRYKAAAA1SqfI+fu+5Ke10NZAAAAUEPIXas3mNkFmZ8vNLPrOy0VAAAAKoVcWn2cu381/cHdvyLp+zsrEQAAAIKEBLkFM7sw/cHMHqawr/YCAABAh0IC2b+V9Gdm9n8lP/+4JL4IDgAAYGAhNzu82cz2JD09mfUCd7+922IBAACgStAl0iS4Ed4AAABGJGSMHAAAAEaIIAcAABApghwAAECkCHIAAACRIsgBAABEiiAHAAAQKYIcAABApAhyAAAAkSLIAQAARIogBwAAECmCHAAAQKQIcgAAAJEiyAEAAESKIAcAABApghwAAECkCHIAAACRIsgBAABEiiAHAAAQKYIcAABApAhyAAAAkSLIAQAARIogBwAAECmCHAAAQKQIcgAAAJEiyAEAAESKIAcAABApghwAAECkCHIAAACRIsgBAABEiiAHAAAQKYIcAABApAhyAAAAkSLIAQAARGqQIGdmrzazvzSzj5rZ75vZBZnfvdLM7jCzT5nZMzPzr07m3WFmr8jMf5SZ3ZLMf5uZHet5cQAAAAYxVI/czZIe6+6Pk/RXkl4pSWZ2haQXSXqMpKsl/R9mtmhmi5J+R9KzJF0h6aeS10rSb0n6bXf/XklfkfSyXpcEAABgIIMEOXf/Y3e/N/nxg5IuTf5/jaS3uvs33f1vJN0h6cnJdIe7f8bdvyXprZKuMTOT9HRJ70j+/gZJz+9pMQAAAAY1hjFyL5X0R8n/L5H02czv7krmFc1flvTVTChM509kZhtmtmdme2fOnGmp+AAAAMM40tUbm9l7Jf29Cb/adPd3Ja/ZlHSvpN2uypHl7tuStiVpbW3N+/hMAACArnQW5Nz9GWW/N7N/Ium5kq509zRU3S3psszLLk3mqWD+PZIuMLMjSa9c9vUAAAAzbai7Vq+W9K8kPc/dDzK/ulHSi8zsuJk9StLlkv5c0ockXZ7coXpMhzdE3JgEwA9IemHy99dJeldfywEAADCkznrkKrxO0nFJNx/er6APuvvPuPsnzOztkm7X4SXXn3X370iSmf0LSTdJWpR0vbt/InmvX5L0VjP73yTdJukN/S4KAADAMOyBq5rzZW1tzff29oYuBgAAQCUzu9Xd1/Lzx3DXKgAAABogyAEAAESKIAcAABApghwAAECkCHIAAACRIsgBAABEiiAHAAAQKYIcAABApAhyAAAAkSLIAQAARIogBwAAECmCHAAAQKQIcgAAAJEiyAEAAESKIAcAABApghwAAECkCHIAAACRIsgBAABEiiAHAAAQKYIcAABApAhyAAAAkSLIAQAARIogBwAAECmCHAAAQKQIcgAAAJEiyAEAAESKIAcAABApghwAAECkCHIAAACRIsgBAABEiiAHAAAQKYIcAABApAhyAAAAkSLIAQAARIogBwAAECmCHAAAQKQIcgAAAJEiyAEAAESKIAcAABApghwAAECkCHIAAACRIsgBAABEiiAHAAAQKYIcAABApAhyAAAAkSLIAQAARIogBwAAECmCHAAAQKQIcgAAAJEiyAEAAESKIAcAABApghwAAECkCHIAAACRGiTImdmvm9lHzezDZvbHZvZdyXwzs9ea2R3J75+Y+ZvrzOzTyXRdZv6TzOxjyd+81sxsiGUCAADo21A9cq9298e5+xMk/aGkX0nmP0vS5cm0Ien1kmRmD5P0KklPkfRkSa8yswuTv3m9pH+a+bure1oGAACAQQ0S5Nz965kfT0jy5P/XSHqzH/qgpAvM7JGSninpZnf/srt/RdLNkq5OfvcQd/+gu7ukN0t6fm8LAgAAMKAjQ32wmW1JerGkr0n6kWT2JZI+m3nZXcm8svl3TZgPAAAw8zrrkTOz95rZxydM10iSu2+6+2WSdiX9i67KkSvThpntmdnemTNn+vhIAACAznTWI+fuzwh86a6k9+hwDNzdki7L/O7SZN7dkn44N/9PkvmXTnh9UZm2JW1L0tramhe9DgAAIAZD3bV6eebHayT9ZfL/GyW9OLl79amSvubun5N0k6SrzOzC5CaHqyTdlPzu62b21ORu1RdLeld/SwIAADCcocbI/aaZPVrSfZL2Jf1MMv89kp4t6Q5JB5JeIknu/mUz+3VJH0pe92vu/uXk//9c0psknSfpj5IJAABg5tnhzZ7zZ21tzff29oYuBgAAQCUzu9Xd1/Lz+WYHAACASBHkAAAAIkWQAwAAiBRBDgAAIFIEOQAAgEgR5AAAACJFkAMAAIgUQQ4AACBSBDkAAIBIEeQAAAAiRZADAACIFEEOAAAgUgQ5AACASBHkAAAAIkWQAwAAiBRBDgAAIFIEOQAAgEgR5AAAACJFkAMAAIgUQQ4AACBSBDkAAIBIEeQAAAAiRZADAACIFEEOAAAgUgQ5AACASBHkAAAAIkWQAwAAiBRBDgAAIFIEOQAAgEgR5AAAACJFkAMAAIgUQQ4AACBSBDkAAIBIEeTQud3dXa2urmphYUGrq6va3d0dukgAAMyEI0MXALNtd3dXGxsbOjg4kCTt7+9rY2NDkrS+vj5k0QAAiB49cujU5ubm/SEudXBwoM3NzYFKBADA7CDIoVOnT5+uNR8AAIQjyKFTJ0+erDUfAACEI8ihU1tbW1paWjpr3tLSkra2tgYqEQAAs4Mgh06tr69re3tbKysrMjOtrKxoe3ubGx0AAGiBufvQZRjE2tqa7+3tDV0MAACASmZ2q7uv5efTIwcAABApghwAAECkCHIAAACRIsgBAABEiiAHAAAQKYIcAABApAhyAAAAkSLIAQAARIogBwAAECmCHAAAQKQIcgAAAJEiyAEAAETK3H3oMgzCzM5I2u/4Yy6S9KWOPwP1sV3Gh20yTmyX8WGbjFMf22XF3S/Oz5zbINcHM9tz97Why4GzsV3Gh20yTmyX8WGbjNOQ24VLqwAAAJEiyAEAAESKINet7aELgInYLuPDNhkntsv4sE3GabDtwhg5AACASNEjBwAAECmCHAAAQKQIch0xs6vN7FNmdoeZvWLo8swLM7vMzD5gZreb2SfM7OeS+Q8zs5vN7NPJvxcm883MXptsp4+a2ROHXYLZZWaLZnabmf1h8vOjzOyWZN2/zcyOJfOPJz/fkfx+ddCCzzAzu8DM3mFmf2lmnzSzp7GvDM/MfiFpvz5uZm8xswexv/TLzK43sy+a2ccz82rvG2Z2XfL6T5vZdV2UlSDXATNblPQ7kp4l6QpJP2VmVwxbqrlxr6SXu/sVkp4q6WeTdf8KSe9z98slvS/5WTrcRpcn04ak1/df5Lnxc5I+mfn5tyT9trt/r6SvSHpZMv9lkr6SzP/t5HXoxmsk/d/u/j9IerwOtw/7yoDM7BJJ/4ukNXd/rKRFSS8S+0vf3iTp6ty8WvuGmT1M0qskPUXSkyW9Kg1/bSLIdePJku5w98+4+7ckvVXSNQOXaS64++fc/S+S/39DhwemS3S4/m9IXnaDpOcn/79G0pv90AclXWBmj+y31LPPzC6V9BxJv5v8bJKeLukdyUvy2yTdVu+QdGXyerTIzB4q6YckvUGS3P1b7v5Vsa+MwRFJ55nZEUlLkj4n9pdeufufSvpybnbdfeOZkm529y+7+1ck3axzw+HUCHLduETSZzM/35XMQ4+SSwzfL+kWSY9w988lv/q8pEck/2db9ePfS/pXku5Lfl6W9FV3vzf5Obve798mye+/lrwe7XqUpDOS3phc8v5dMzsh9pVBufvdkv6NpNM6DHBfk3Sr2F/GoO6+0cs+Q5DDTDKzB0v6PUk/7+5fz/7OD5+5w3N3emJmz5X0RXe/deiy4CxHJD1R0uvd/fsl/a0euFQkiX1lCMmlt2t0GLS/S9IJddCLg+mMad8gyHXjbkmXZX6+NJmHHpjZUR2GuF13f2cy+wvpZaDk3y8m89lW3ftBSc8zszt1OMzg6Tocm3VBculIOnu9379Nkt8/VNI9fRZ4Ttwl6S53vyX5+R06DHbsK8N6hqS/cfcz7v5tSe/U4T7E/jK8uvtGL/sMQa4bH5J0eXKX0TEdDlS9ceAyzYVkbMgbJH3S3f9d5lc3SkrvGLpO0rsy81+c3HX0VElfy3SdowXu/kp3v9TdV3W4L7zf3dclfUDSC5OX5bdJuq1emLx+FGe+s8TdPy/ps2b26GTWlZJuF/vK0E5LeqqZLSXtWbpd2F+GV3ffuEnSVWZ2YdLTelUyr1V8s0NHzOzZOhwXtCjpenffGrZE88HM/kdJ/1XSx/TAeKxf1uE4ubdLOilpX9JPuPuXk4bydTq8dHEg6SXuvtd7weeEmf2wpF909+ea2XfrsIfuYZJuk3Stu3/TzB4k6f/U4fjGL0t6kbt/ZqAizzQze4IOb0A5Jukzkl6iwxN89pUBmdn/KukndXgX/m2S/icdjq1if+mJmb1F0g9LukjSF3R49+kfqOa+YWYv1eExSJK23P2NrZeVIAcAABAnLq0CAABEiiAHAAAQKYIcAABApAhyAAAAkSLIAQAARIogBwAAECmCHAAAQKQIcgCQYWarZvZJM/tPZvYJM/tjMztvwut+3Mw+bmYfMbM/TeYtmtmrzexDZvZRM/tnyXwzs9eZ2afM7L1m9h4ze2H+PQGgLoIcAJzrckm/4+6PkfRVSf9owmt+RdIz3f3xkp6XzHuZDr+e5wck/YCkf2pmj5L0Y5IeLekKSS+W9A+6LT6AeXGk+iUAMHf+xt0/nPz/VkmrE17z3yS9yczersMvNpcOv0vxcZnetofqMBT+kKS3uPt3JP2/Zvb+rgoOYL4Q5ADgXN/M/P87ks65tOruP2NmT5H0HEm3mtmTJJmk/9ndz/pi7OS7lwGgdVxaBYAGzOx73P0Wd/8VSWckXSbpJkmnzOxo8prvM7MTkv5U0k8mY+geKelHBis4gJlCjxwANPNqM7tch71w75P0EUkf1eFl2L8wM9NhwHu+pN+X9HRJt0s6LenP0jcxs1+TtOfuN/ZZeACzwdx96DIAwFwxszdJ+kN3f8fQZQEQNy6tAgAARIoeOQAAgEjRIwcAABApghwAAECkCHIAAACRIsgBAABEiiAHAAAQqf8fKz8UjD+gt9kAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#O gráfico abaixo mostra os valores reais em preto e os valores prditos em azul\n",
        "npredicts=1000\n",
        "pyplot.figure(figsize=(10,7)) \n",
        "pyplot.scatter(np.arange(0,y_label.shape[0],1)[0:npredicts],np.array(y_label)[0:npredicts], color = 'black')\n",
        "pyplot.plot(np.arange(0,predict.shape[0],1)[0:npredicts], np.array(predict)[0:npredicts], color = 'blue', linewidth=2)\n",
        "pyplot.xlabel('n seq.')\n",
        "pyplot.ylabel('consumo l/h')\n",
        "pyplot.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
